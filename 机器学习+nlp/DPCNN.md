# Deep Pyramid Convolutional Neural Networksfor Text Categorization

note：有些内容忘记从哪里看的，复制的，侵删！



深度金字塔卷积神经网络

```
在DPCNN的工作之前，研究者们认为，word-level词级embedding优于char-level字级（严格意义上来说，elmo、gpt-2、bert、xlnet等是字(char)、词(word)和句子(sentence)级别的混合体，所以不算）；CNN在图像领域的成功也表明，深度卷积神经网络能够提取更加复杂和高级的特征，尤其是深度残差网络(ResNet)等的流行。那么，深度卷积神经网络在自然语言处理NLP领域，究竟有没有优势呢?
	elmo、gpt-2、bert和xlNet的出现，说明深度神经网络在自然语言处理NLP领域，也是很有必要的，不过，这个主角可能不是CNN，而是Attention。
	NLP任务的字符、词等是离散的数据结构呀，这和图像连续的像素、颜色等具有本质的区别。
```

```
通过TextCNN、DCNN、Bi-LSTM，我们已经知道，句子顺序对于自然语言处理NLP任务具有很非常重要的作用，从直观上看，道理也是这样子的，尤其是对于短文本来说。
	 在DPCNN之前，研究者们已经证明在char-level的深度卷积神经网络中，更深的模型效果更好，不过，由于word词太多、深度网络太复杂，常常会导致模型过大、参数太多、运行较慢、梯度爆炸和消失等问题。
```

![image-20201117134419997](https://i.loli.net/2020/11/17/lVpZ3bQETP8aMW4.png)

DPCNN主要由  A. **Redion embedding层**(文本区域嵌入层)、 

​                  B. **两个convolution block**（每层block由两个固定卷积核为3的conv卷积函数构成）（两个block构建的层可以通过pre-activation直接连接）、

​                  C. **Repeat结构**，与B很相似，只不过在conv之前、pre-activate之后加了个Max-polling层

等长卷积：

```
不同于TextCNN的窄卷积(VALID)，也不同于DCNN中的宽卷积(wide)，DPCNN中的卷积使用等长卷积(SAME)，顾名思义，就是输出的卷积长度与输入的一样，步长一样是1，不同的是padding补零，为两端补零pad_size=(m-1)/2 ，m为卷积核尺寸。
```

池化：

```
不同于TextCNN的窄卷积(VALID)，也不同于DCNN中的宽卷积(wide)，DPCNN中的卷积使用等长卷积(SAME)，顾名思义，就是输出的卷积长度与输入的一样，步长一样是1，不同的是padding补零，为两端补零pad_size=(m-1)/2 ，m为卷积核尺寸。
```

**shortcut connect**

```
ResNet中思想，shortcut connect残差连接等。使用加法的z+f(z)等长卷积进行近路连接，从而极大地避免了梯度消失问题，右使用线性(Linear)激活函数(activate)，降低计算复杂度等。

```



# 展开的想法：

在ACL2017以前，word-level的文本分类模型（以单词为语义单位）自2014Kim等人提出的TextCNN模型后，就没有再出现过显著有效的CNN系模型，尤其是深层模型。

本篇文章：

严格意义上第一个word-level的广泛有效的深层文本分类卷积神经网

这篇论文里还使用了*two-view embedding*来进一步提升模型性能，这里作者将TextCNN的包含多尺寸卷积滤波器的卷积层的卷积结果称之为Region embedding，意思就是对一个文本区域/片段（比如3gram）进行一组卷积操作后生成的embedding。

对一个3gram进行卷积操作时可以有两种选择，一种是保留词序，也就是设置一组size=3*D的二维卷积核对3gram进行卷积（其中D是word embedding维度）；还有一种是不保留词序（即使用词袋模型），即首先对3gram中的3个词的embedding取均值得到一个size=D的向量，然后设置一组size=D的一维卷积核对该3gram进行卷积。显然TextCNN里使用的是保留词序的做法，而DPCNN使用的是词袋模型的做法，

TextCNN恰好可以较好的利用词向量中的知识（近义关系）罢了。这意味着，经典模型里难以学习的远距离信息（如12gram）在TextCNN中依然难以学习。**那么这些长距离复杂模式如何让网络学习到呢？**

显然，要么加深全连接层，要么加深卷积层。加深哪个更好呢？

对于文本分类来说：强特征隐藏的非常浅，“开心”，，“愤怒”这些词

但总有一些隐晦的样本：明褒暗贬，当然是加强分类层呀～让分类层给每个情感词的重要性打分！让分类层去描述中性词与情感词的作用关系，让分类层找出情感词共现的分类规律，反正我提取特征的层已经给你把特征提取出来了哼。

近年来已经有不止一篇paper专门研究文本分类里的卷积层深浅的有效性问题，[2]使用[4]这个简单的一层卷积+线性分类就已经足以让文本分类取得非常好的效果了，而作者将DenseNet直接拿来用发现效果很差劲，性能如下表。

![image-20200817132243471](https://i.loli.net/2020/11/17/c6rsURJAldgWnYS.png)

[3]类似。同时[4]提出的DAN（Deep Averaging Nets）恰好是指出特征随便提取一下（仅使用神经词袋模型），然后分类层疯狂加深，则发现可以大幅提升文本语义的描述能力，如图2。

![image-20201117134448808](https://i.loli.net/2020/11/17/uHDfwvZsbGjThQP.png)

同时我自己也做了一下相关研究，发现仅使用1层CNN+3层全连接即在大多数文本分类数据集上超过了当时的state-of-art，当时做实验时首先用的是1层CNN1层全连接，也就是2014Kim[5]的这个经典结构，然后加深CNN却发现模型性能不升反降，但是加深全连接却明显提升，加到3层时提升的效果很显著，4层又能得到小幅提升，5层出现下降，6层下降明显。所以在出现提取了大量强特征的情况时，考虑加深一下分类层还是比较有意义的尝试。



## “职责转移”问题

众所周知，把分类层做深是计算代价很高、学习效率很低的操作，而卷积层则高效的多。既然加深分类层获得性能增益的原理是学习特征之间的关联与权重，也就是类似于将特征进行加权以及合并。那么其实这个过程可以提到卷积层进行。

想一想，如果特征“不要”和“太好”在文本里先后邻接出现的话，那显然这两个强特征可以合并为“不要太好”这个强特征，类似于“很好”的语义。同样，特征之间还可以通过句法关系来进一步合并，只不过这种句法关系可能空间上相隔较远，需要多叠几层卷积层才方便捕捉。通过卷积层来进行特征合并更加接近样本里本来的特征作用关系，因此这种操作在这一层面上反而是比加深分类层更加高效明智的。

也正因为如此，随便一加深卷积层是容易起反效果的，这时既难以很准确的合并特征，还可能产生了一堆干扰分类的噪声特征，正如[2]中展现的DenseNet直接从图像领域拿到文本领域是非常不work的，而经过精心设计的DPCNN[6]就能取得显著的分类性能提升。我猜这也是为什么时隔这么久才在文本分类领域出现一个word-level “ResNet”的原因吧。

# 等长卷积

通过填充来实现输出和输入等长

意义：

既然输入输出序列的位置数一样多，我们将输入输出序列的第n个embedding称为第n个词位，那么这时size为n的卷积核产生的等长卷积的意义就很明显了，那就是**将输入序列的每个词位及其左右((n-1)/2)个词的上下文信息压缩为该词位的embedding**，也就是说，产生了每个词位的被上下文信息修饰过的更高level更加准确的语义。

回到DPCNN上来。我们想要克服TextCNN的缺点，捕获长距离模式，显然就要用到深层CNN啦。那么直接等长卷积堆等长卷积可不可以呢

然这样会让每个词位包含进去越来越多，越来越长的上下文信息，但是这样效率也太低了喂，显然会让网络层数变得非常非常非常深。

等长卷积堆等长卷积会让每个词位的embedding描述语义描述的更加丰富准确，那么**当然我们可以适当的堆两层来提高词位embedding的表示的丰富性。**



固定feature map的数量：

在表示好每个词位的语义后，其实很多邻接词或者邻接ngram的词义是可以合并的，例如“小娟 姐姐 人 不要 太好”中的“不要”和“太好”虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，这样完全可以把“不要”和“太好”的语义进行合并哇。同时，合并的过程完全可以在原始的embedding space中进行的，毕竟原文中直接把“不要太好”合并为“很好”是很可以的哇，**完全没有必要动整个语义空间**。

而实际上，相比图像中这种从“点、线、弧”这种low-level特征到“眼睛、鼻子、嘴”这种high-level特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（1gram）到短语再到3gram、4gram的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种”语义取代“现象（例如“鼻子”的语义可以被”弧线“的语义取代嘛？）。

因此（划重点），DPCNN与ResNet很大一个不同就是，在DPCNN中固定死了feature map的数量，也就是固定住了embedding space的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接ngram）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的哦，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。而ResNet则是不断的改变语义空间，使得图像的语义随着网络层的加深也不断的跳向更高level的语义空间。



# 1/2池化层

每经过一个size=3, stride=2（大小为3，步长为2）的池化层（以下简称1/2池化层），序列的长度就被压缩成了原来的一半（请自行脑补）。这样同样是size=3的卷积核，每经过一个1/2池化层后，其能感知到的文本片段就比之前长了一倍。

# 残差连接

网络加深，存在梯度弥散的问题。

ResNet中提出的shortcut-connection/skip-connection/residual-connection（残差连接）就是一种非常简单、合理、有效的解决方案。，**既然每个block的输入在初始阶段容易是0而无法激活，那么直接用一条线把region embedding层连接到每个block的输入乃至最终的池化层/输出层不就可以啦！**